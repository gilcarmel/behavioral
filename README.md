For this project, I designed and trained a model that predicts steering angles based on an image from a simulated car's front-facing camera.

Data Collection
I collected training data (images and their corresponding steering angles) by driving the simulated car for a few laps down the center of the lane. I added a lot of examples of recentering the car from both the left and right shoulder. This is necessary to help the model recover when the car inevitably starts to drift (a model trained purely on center-lane driving would not know how to recover).
I used a gaming controller to drive, so that the training data contained smooth steering angles (as opposed to the discreet values of -1, 0, and 1 generated by driving with the keyboard). 

I found it hard to control the car perfectly and some bad driving examples definitely snuck in and polluted my training data. For production models, I would need an easy way to discard bad training data - perhaps through a visual editor tool. 

Preprocessing

Input images are preprocessed prior to training and inference:
* YUV conversion: My models performed better with YUV images (vs RGB). I found this surprising - I would think the convolutional layers of my model would in a way "figure out" the correct color space.
* Scaling/cropping: images were scaled down to 30% scale, and cropped the top 30% (mostly sky). After visually examining the scaled/cropped images, I suspected they still conveyed enough information to make a correct steering prediction (this was confirmed in practice later). The smaller images dramatically improved network training time, allowing for more experimentation with preprocessing, architecture, and hyperparemeters.
* Horizontal flipping: The data was collected driving one way around the looped track, resulting in a much higher proportion of left turns vs right turns. After noticing that the model was performing fairly well on left turns but poorly on sharp right turns, I doubled the data set by flipping each image and negating its corresponding steering angle.

Network Architecutre
After initially trying [NVIDIA's network structure](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)with the original image size (320x160), I decided to try a simpler network to further reduce training times. My intuition was that NVIDIA's model had to deal with a much wider variety of environments and lighting conditions than our simple simulator.

I ended up with the following architecture:

    (input 96x48x3) -->
      (crop to 96x34x3) --> 
	     (conv to 48x17x24) --> (maxpool to 24x8x24) -> relu -> dropout(0.2) -->
            (conv to 24x8x36) --> (maxpool to 12x4x36) -> relu -> dropout(0.2) -->
               (flatten) --> (fully connected to 100) --> dropout(0.2) --> relu -->
    	           (flatten) --> (fully connected to 10) --> relu -->
    	              (fully connected to 1) --> prediction:

Here is the summary as printed by Keras:

    Layer (type)                     Output Shape          Param #     Connected to                     
    ====================================================================================================
    cropping2d_1 (Cropping2D)        (None, 34, 96, 3)     0           cropping2d_input_1[0][0]         
    ____________________________________________________________________________________________________
    convolution2d_1 (Convolution2D)  (None, 17, 48, 24)    1824        cropping2d_1[0][0]               
    ____________________________________________________________________________________________________
    maxpooling2d_1 (MaxPooling2D)    (None, 8, 24, 24)     0           convolution2d_1[0][0]            
    ____________________________________________________________________________________________________
    activation_1 (Activation)        (None, 8, 24, 24)     0           maxpooling2d_1[0][0]             
    ____________________________________________________________________________________________________
    dropout_1 (Dropout)              (None, 8, 24, 24)     0           activation_1[0][0]               
    ____________________________________________________________________________________________________
    convolution2d_2 (Convolution2D)  (None, 8, 24, 36)     7812        dropout_1[0][0]                  
    ____________________________________________________________________________________________________
    maxpooling2d_2 (MaxPooling2D)    (None, 4, 12, 36)     0           convolution2d_2[0][0]            
    ____________________________________________________________________________________________________
    dropout_2 (Dropout)              (None, 4, 12, 36)     0           maxpooling2d_2[0][0]             
    ____________________________________________________________________________________________________
    activation_2 (Activation)        (None, 4, 12, 36)     0           dropout_2[0][0]                  
    ____________________________________________________________________________________________________
    flatten_1 (Flatten)              (None, 1728)          0           activation_2[0][0]               
    ____________________________________________________________________________________________________
    hidden1 (Dense)                  (None, 100)           172900      flatten_1[0][0]                  
    ____________________________________________________________________________________________________
    dropout_3 (Dropout)              (None, 100)           0           hidden1[0][0]                    
    ____________________________________________________________________________________________________
    activation_3 (Activation)        (None, 100)           0           dropout_3[0][0]                  
    ____________________________________________________________________________________________________
    hidden3 (Dense)                  (None, 10)            1010        activation_3[0][0]               
    ____________________________________________________________________________________________________
    activation_4 (Activation)        (None, 10)            0           hidden3[0][0]                    
    ____________________________________________________________________________________________________
    output (Dense)                   (None, 1)             11          activation_4[0][0]               
    ====================================================================================================
    Total params: 183557
    ____________________________________________________________________________________________________


Training
- Ran on AWS
- random seeding - not working?
